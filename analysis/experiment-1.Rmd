---
title: "Experiment 1"
output: html_notebook
---

## Setup 

This experiment will require the results of the monte-carlo runs produced by the
`experiment-1*.R` programs in this directory.  Those programs produce the
`*.rds` files loaded in this section.

```{r loaddata}
library('hectorcal')
library('hector')
library('coda')
library('dplyr')
library('foreach')

mcrslts_mesa_full <- readRDS('mcrslts_mesa_full.rds')
mcmc_mesa_full <- metrosamp2coda(mcrslts_mesa_full)

mcrslts_hi10 <- readRDS('mcrslts_hi10.rds')
mcmc_hi10 <- metrosamp2coda(mcrslts_hi10)

mcrslts_wide_priors <- readRDS('mcrslts_wide_priors.rds')
mcmc_wide_priors <- metrosamp2coda(mcrslts_wide_priors)

ncore <- 8
doParallel::registerDoParallel(cores=ncore)

```

## Introduction

In the runs that we did for the poster back in the fall, it looked as if the
runs might not be covering the full range of the CMIP results.  Moreover, there
seemed to be a marked falloff in the density closer to the edge of the CMIP
range, despite the fact that the likelihood function was flat over much of this
range.  A third observation was that the hector outputs for most of the sampled
parameters seemed to be approximately parallel to one another; there were very
few runs, for example, that started high and finished low, or vice versa.

All of these observations were made using the spaghetti plots of hector outputs
for a subsample of the Monte Carlo parameter samples, which is not a very precise
way of diagnosing these effects.  Therefore, the purpose of this experiment is to 
examine these matters more carefully to ensure that the Monte Carlo sampling is 
behaving as expected.

## Part A: Do the results cover the full range?

In this part we reran the full-range Monte Carlo calculations.  The code for 
performing this run is in `experiment-1A.R`; the results were saved as 
`mcrslts_mesa_full.rds`.

We were having trouble getting good mixing in these calculations.  Different 
chains were converging to different, non-overlapping regions of the parameter
space.  We were able to mitigate this problem by increasing the scale factor 
for the proposal distribution.  This had the side effect of introducing a _lot_
of autocorrelation into the Markov chains, necessitating longer runs than we
had anticipated.  Even with 8 chains of 10,000 samples each, our effective 
sample size is only a few hundred.  Together with the Gelman-Rubin diagnostic,
this suggests that our production runs could stand to be a bit longer; however,
these results should suffice for the purposes of this experiment.

```{r diag.1a}
autocorr.diag(mcmc_mesa_full)
effectiveSize(mcmc_mesa_full)
gelman.diag(mcmc_mesa_full)
```

The basic results of the calculation are below.  The marginal densities are
consistent with what we saw in the poster results.  

```{r rslt.1a}
summary(mcmc_mesa_full)
densplot(mcmc_mesa_full)
```

Do these outputs in fact cover the whole range of the CMIP outputs?  We can compare
the ranges in the comparison data to the distribution of values that come out of 
the hector runs from our sampled parameters at selected times.

```{r rangecomp}
testtimes <- c(2006, 2050, 2100)
pnames <- c(ECS(), AERO_SCALE(), DIFFUSIVITY(), BETA(), Q10_RH(), PREINDUSTRIAL_CO2())
comp_esmrcp85 <- filter(esm_comparison, experiment=='esmrcp85')
esmrcp85_ini <- system.file("input/hector_rcp85.ini", package = "hector")
hcores <- lapply(1:ncore, function(i){newcore(esmrcp85_ini)})
samps_all <- do.call(rbind, lapply(mcrslts_mesa_full, function(x){x$samples}))

filter(comp_esmrcp85, year %in% testtimes)
hector_output_stats(samps_all, hcores, pnames, nsamp=1000, 
                    times=testtimes, quantiles=c(0,0.1, 0.5, 0.9, 1))
```

The `mina` and `maxb` columns give the range of the ensemble, and as can be seen
in the quantile stats, our minimum and maximum values are actually a tiny bit
outside the CMIP range. This is because our likelihood function is not a sharp
cutoff, so the values can be a little outside the range at some times,
especially if they were in range over the rest of the run.  So, we _do_ seem to
be producing the full temperature and CO2 range.

On the other hand, looking at the 10-90 percentile range for temperature shows
that for our MC samples it is a little narrower than the corresponding range for
the CMIP models.  This indicates that the probability density is falling off as
you get to the upper and lower edge of the range, relative to what we are seeing
in the CMIP models.  We also observed this in the poster results, and itâ€™s still
not entirely clear why that should be, since the likelihood function is mostly
flat within the CMIP range.  We do have some hypotheses; the leading one is that
the volume of the parameter space that can produce runs in these ranges is rather
small, making it hard to land in that part of the output space.

Interestingly, our 10-90 percentile range for CO2 seems to match the
corresponding CMIP range more closely than is the case for temperature.  This
observation supports the restricted volume hypothesis.  CO2 outputs depend only
on the parameters that affect the carbon cycle, while temperature outputs depend
on all six parameters jointly.  Thus, in order to get temperatures toward, say, 
the high end of the range, you need a confluence of high CO2 values and high
temperature parameters.  By contrast, to get high CO2 values, you need only 
have the right carbon cycle parameters; the temperature parameters can be anything.
The part of the parameter space that produces high temperature values is therefore
approximately a subset of the part that produces high CO2 values.

Looking at a sample of the hector output traces is also instructive.
```{r spaghetti.1a}
spaghetti_plot(mcrslts_mesa_full, 512, hcores, pnames, alpha=0.1) + ggplot2::ggtitle('Full range')
```

First, we can see that the lack of any historical constraint is allowing some
slightly absurd models into the sample.  When we rerun with our new dataset in
place, we can expect the parameter distributions to be more constrained (though
the difference may or may not be apparent on casual inspection.)  Another thing
that seems apparent is that the the concern about the lack of crossing traces, 
at least for temperature, seems to have been unfounded.  The CO2 results, on 
the other hand, show a lot less diversity in the outputs.  Evidently, the model
doesn't have as much flexibility in producing CO2 output curves with dramatically
different shapes.

## Part B: Why does the density of the traces fall off near the edge of the range?

To investigate the falloff toward the edge of the range, we set up a Monte Carlo 
run where we force the temperature to end the century in the upper decile of the 
range, while the final CO2 concentration is constrained to the full range. 
Temperature and CO2 values were also constrained to the full range in years 2006
and 2050.  As might be expected, the end of century temperature constraint leave us with 
a changed set of viable parameters.

```{r compare.1a.1b}
summary(mcmc_mesa_full)   # Results from the previous run
summary(mcmc_hi10)        # Results from this run
```

As might be expected, the major difference here is that the equilibrium climate
sensitivity, $S$, is substantially higher. The aerosol scaling factor is also 
pushed higher, and extremely low values for that parameter are eliminated entirely.
However, there _doesn't_ seem to be any evidence that the parameters need to be 
fine-tuned to land in this range of outputs.  In particular, the width of the 
distributions of the temperature parameters (whether measured by standard 
deviation or by interquartile range) isn't much smaller than what we saw in Part
A.  Likewise, apart from a slight increase in $\beta$, the values of the carbon
cycle parameters aren't much different in the Part B run, as compared to Part A.
This translates to CO2 concentrations that are roughly consistent between the
two runs.

```{r compare_output.1a.1b}
samps_hi10 <- do.call(rbind, lapply(mcrslts_hi10, function(x){x$samples}))
hector_output_stats(samps_all, hcores, pnames, nsamp=1000, times=testtimes, quantiles=c(0,0.1, 0.5, 0.9, 1))
hector_output_stats(samps_hi10, hcores, pnames, nsamp=1000, times=testtimes, quantiles=c(0,0.1, 0.5, 0.9, 1))
```

The output traces don't show anything particularly unexpected.

```{r spaghetti.1b}
spaghetti_plot(mcrslts_hi10, 512, hcores, pnames, alpha=0.1) + ggplot2::ggtitle('Upper decile, final year only')
```

Based on these results, the fine-tuning hypothesis doesn't seem viable to me anymore.
Our next best hypothesis is that this is being driven by our prior.  In particular, 
considor the prior for climate sensitivity, $S$.

```{r ecsprior}
ps <- function(x) {dlnorm(x, log(3.0), log(3.0))}
curve(ps, from=0, to=10, ylab='p(S)', xlab='S')
```

The prior probability mass over the range 2.5--3.5 (i.e., the values favored in the full-range calculation) 
is `r signif(integrate(ps, 2.5, 3.5)$value, 2)`, while the prior probability mass over the 5.5--6.5 range
favored by the high-decile calculation is `r signif(integrate(ps, 5.5, 6.5)$value, 2)`.  This difference in prior 
probability is not enough to exclude these higher temperature runs, but it could cause the reduction in probability
density we have observed in the output traces.  We could test this by broadening the priors to make them 
less informative and rerunning the full-range calibration test.

## Part C: Can the Priors explain the edge density falloff?

To investigate this hypothesis we reran the calculation with part A, but with the priors replaced by very 
non-informative distributions.  Specifically, we made them all normal with $\sigma = 10$, except for the 
preindustrial CO2 prior, which had $\sigma = 60$, owing to the larger scale of that parameter.

The first thing to observe is that the convergence for the Markov chains in this part of the experiment
is marginal at best.  Looking at the trace plots below, we see that several of the variables have chains 
with lengthy excursions into very large or very small parameter values.  The chains are also highly correlated;
despite running 8 chains of 10,000 samples each, the total effective sample count for $S$ is just 
`r round(neff(mcmc_wide_priors)[1])`.  This poor convergence is a result of the lack of stabilizing influence 
of weakly-informative priors.  If we were using this run for production results, we would want to try to 
get better convergence, but for purposes of comparing the edge behavior, what we have here should suffice.

```{r partc.trace}
plot(mcmc_wide_priors)
```

The density plots side-by-side with the trace plots give some idea of the effect
of the priors on the posterior distributions.  Comparing these to the density plots
from Part A, we can see that $S$ extends to much larger and smaller values than were
seen in part A, as do $\alpha$ (aerosol scaling), $\kappa$ (diffusivity), and $Q_{10}$.
Looking at the distribution statistics for part A and part C gives us a quantitative
version of the same story.

```{r parta.partc.dist}
summary(mcmc_mesa_full)
summary(mcmc_wide_priors)
```

_remarks on summary stats_

Based on these results, we would expect to see a less pronounced density falloff 
in the hector output traces.

```{r partc.spaghetti}
spaghetti_plot(mcrslts_wide_priors, 512, hcores, pnames, alpha=0.1) + ggplot2::ggtitle('Full range calibration with wide priors')
```

It's hard to say for sure if there is less density falloff than there was in the part A
runs.  Only the traces that end up between temperatures of 3.4 and 6.25 count for this
purpose; outside of that range the density is falling off because the mesa function is 
saying that our results don't belong out there.

```{r partc.hstats}
samps_wide_priors <- do.call(rbind, lapply(mcrslts_wide_priors, function(x){x$samples}))
hector_output_stats(samps_all, hcores, pnames, nsamp=1000, times=testtimes, quantiles=c(0,0.1, 0.5, 0.9, 1))
hector_output_stats(samps_wide_priors, hcores, pnames, nsamp=1000, times=testtimes, quantiles=c(0,0.1, 0.5, 0.9, 1))
```

Let's do what we probably should have done from the start, namely, to compute kernel 
density functions for the Hector outputs directly.

```{r partc.outsamps}
hs_all <- run_hector_samples(samps_all, 2048, hcores, c(2050, 2100), pnames, c(GLOBAL_TEMP(), ATMOSPHERIC_CO2()))
hs_all$runtype <- 'part A'
hs_wp  <- run_hector_samples(samps_wide_priors, 2048, hcores, c(2050, 2100), pnames, c(GLOBAL_TEMP(), ATMOSPHERIC_CO2()))
hs_wp$runtype <- 'part C'
hs <- rbind(hs_all, hs_wp)
```

```{r partc.outdens}
ggplot2::ggplot(data=hs, ggplot2::aes(x=value, fill=runtype)) + ggplot2::geom_density(alpha=0.5) +
    ggplot2::facet_wrap(~year+variable, scales='free') + 
    ggthemes::theme_economist() + ggthemes::scale_fill_economist()
```

Now we arrive at the truth of the matter, which is that _despite_ the dramatic
differences in _parameter_ distributions between the two runs, the differences
in _output_ distributions don't look all that different.  The $C_a$ distribution
extends a little further up on the high end, right up to the edge of the full
range value (1142 ppm).  Evidently our prior was causing us to exclude a little
bit of that range.

The $T_g$ values look remarkably similar between the two runs, and in 2100, at 
least, they appear reasonably flat-topped over the CMIP range of 3.36--6.25 deg 
C for that year.  The Part C $C_a$ distribution is also reasonably flat over the
CMIP range. The remaining peakiness in the distributions I'm willing to ascribe 
to less than perfect mixing in the Markov chains.  (One thing we have learned
from this experiment is that we are likely going to have to do more samples than
we expected.) At this point, however, I'm not convinced that there is any
genuine edge falloff to be found.  It's just another artifact of trying to
diagnose this from the spaghetti plots.  (It's worth noting that the spaghetti
plots are very lightly sampled compared to these.)

Final verdict, I think this is a non-issue, except perhaps that it's possibly
indicative of poor convergence in our Monte Carlo runs; however, we have other
ways of diagnosing that.  Unless someone else on the team sees something that
really concerns them, I'm going to consider the myth of edge density falloff to
be busted.

![](busted2.jpg)
